{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45a6e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Installation des bibliothèques nécessaires:\n",
    "!pip install -q langchain transformers sentence-transformers faiss-cpu accelerate torch langchain_community\n",
    "\n",
    "# pour utilisé ChromaDB:\n",
    "!pip install -q chromadb\n",
    "\n",
    "# Téléchargement des dépendances Python:\n",
    "import os\n",
    "import torch\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"Installations et imports terminés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01820fff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION DES COMPOSANTS RAG :\n",
    "\n",
    "FILE_PATH = \"documentation_interne.txt\"\n",
    "\n",
    "# Chargement et Division des données (Chunking):\n",
    "try:\n",
    "    with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        document_content = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERREUR: Le fichier {FILE_PATH} n'a pas été trouvé. Veuillez le téléverser dans Colab.\")\n",
    "    exit()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "docs = text_splitter.create_documents([document_content])\n",
    "print(f\"Document divisé en {len(docs)} segments (chunks).\")\n",
    "\n",
    "# EMBEDDING (Hugging Face Local et Gratuit):\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'} \n",
    ")\n",
    "\n",
    "# CRÉATION DU VECTOR STORE (Mémoire RAG):\n",
    "vector_store = FAISS.from_documents(docs, embedding_function)\n",
    "rag_retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"✅ Base de données FAISS (RAG) créée avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a6fb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION DU LLM (Mistral sur GPU Colab):\n",
    "\n",
    "# Charger Mistral 7B Instruct depuis Hugging Face:\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Utiliser le pipeline Transformers (GPU de Colab)\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        # 'device' est retiré ici pour éviter le conflit\n",
    "    },\n",
    "    device=0,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16} \n",
    ")\n",
    "print(f\"✅ LLM Agent (Mistral-7B-Instruct) chargé depuis Hugging Face sur Colab.\")\n",
    "\n",
    "\n",
    "# --- CONSTRUCTION DE LA CHAÎNE RAG DIRECTE (LCEL) ---\n",
    "\n",
    "RAG_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Tu es un assistant IA professionnel. Utilise UNIQUEMENT le contexte fourni ci-dessous pour répondre à la question. \n",
    "Si la réponse n'est pas dans le contexte, tu dois répondre de manière polie : 'Je suis désolé, cette information spécifique n'est pas disponible dans ma documentation interne.'\n",
    "\n",
    "--- CONTEXTE ---\n",
    "{context}\n",
    "--- FIN CONTEXTE ---\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "# Assemblage de la Chaîne RAG LCEL\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context= (lambda x: x['question']) | rag_retriever \n",
    "    )\n",
    "    | RAG_PROMPT\n",
    "    | llm # Le LLM HuggingFacePipeline est utilisé ici\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- TESTS DE VALIDATION ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST DE LA CHAÎNE RAG DIRECTE SUR COLAB (Validation effectuée)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test 1: Question Interne (utilisation du RAG pour répondre)\n",
    "print(\"\\n--- TEST 1: Question Interne ---\")\n",
    "question_interne = \"Combien de temps faut-il pour finaliser le traitement d'un remboursement ?\"\n",
    "print(f\"Question : {question_interne}\")\n",
    "response_interne = rag_chain.invoke({\"question\": question_interne})\n",
    "print(f\"\\nRÉPONSE RAG : {response_interne}\")\n",
    "\n",
    "# Test 2: Question Généraliste (DOIT répondre 'Information non disponible')\n",
    "print(\"\\n--- TEST 2: Question Générale ---\")\n",
    "question_generale = \"Quel est le plus grand désert du monde ?\"\n",
    "print(f\"Question : {question_generale}\")\n",
    "response_generale = rag_chain.invoke({\"question\": question_generale})\n",
    "print(f\"RÉPONSE RAG : {response_generale}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
