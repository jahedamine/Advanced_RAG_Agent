{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45a6e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Installation des bibliothèques nécessaires\n",
    "!pip install -q langchain transformers sentence-transformers faiss-cpu accelerate torch langchain_community\n",
    "\n",
    "# Si vous avez utilisé ChromaDB, vous pouvez l'installer, mais FAISS est souvent plus simple dans Colab\n",
    "# !pip install -q chromadb\n",
    "\n",
    "# Téléchargement des dépendances Python\n",
    "import os\n",
    "import torch\n",
    "from langchain_community.vectorstores import FAISS # Utilisation de FAISS, plus léger que Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline # Le LLM de Colab\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"Installations et imports terminés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01820fff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- CONFIGURATION DES COMPOSANTS RAG ---\n",
    "\n",
    "FILE_PATH = \"documentation_interne.txt\"\n",
    "\n",
    "# 1. Chargement et Division des données (Chunking)\n",
    "try:\n",
    "    with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        document_content = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERREUR: Le fichier {FILE_PATH} n'a pas été trouvé. Veuillez le téléverser dans Colab.\")\n",
    "    exit()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "docs = text_splitter.create_documents([document_content])\n",
    "print(f\"Document divisé en {len(docs)} segments (chunks).\")\n",
    "\n",
    "# 2. EMBEDDING (Hugging Face Local et Gratuit)\n",
    "# On utilise les embeddings HFE pour encoder les segments\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'} \n",
    ")\n",
    "\n",
    "# 3. CRÉATION DU VECTOR STORE (Mémoire RAG)\n",
    "# FAISS est le plus rapide à initialiser dans Colab\n",
    "vector_store = FAISS.from_documents(docs, embedding_function)\n",
    "rag_retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"✅ Base de données FAISS (RAG) créée avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a6fb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- CONFIGURATION DU LLM (Mistral sur GPU Colab) ---\n",
    "\n",
    "# Charger Mistral 7B Instruct depuis Hugging Face\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Utiliser le pipeline Transformers (qui utilise le GPU de Colab)\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    # Correction : On retire l'argument 'device' du pipeline_kwargs\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        # 'device' est retiré ici pour éviter le conflit\n",
    "    },\n",
    "    # Et on utilise l'argument 'device' natif de la méthode LangChain si nécessaire,\n",
    "    # mais souvent, LangChain le gère automatiquement si un GPU est présent.\n",
    "    # Pour garantir l'utilisation du GPU T4, on passe device=0\n",
    "    device=0, # <-- Ajout de device=0 pour cibler explicitement le premier GPU\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16} \n",
    ")\n",
    "print(f\"✅ LLM Agent (Mistral-7B-Instruct) chargé depuis Hugging Face sur Colab.\")\n",
    "\n",
    "\n",
    "# --- CONSTRUCTION DE LA CHAÎNE RAG DIRECTE (LCEL) ---\n",
    "\n",
    "RAG_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Tu es un assistant IA professionnel. Utilise UNIQUEMENT le contexte fourni ci-dessous pour répondre à la question. \n",
    "Si la réponse n'est pas dans le contexte, tu dois répondre de manière polie : 'Je suis désolé, cette information spécifique n'est pas disponible dans ma documentation interne.'\n",
    "\n",
    "--- CONTEXTE ---\n",
    "{context}\n",
    "--- FIN CONTEXTE ---\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "# Assemblage de la Chaîne RAG LCEL\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context= (lambda x: x['question']) | rag_retriever \n",
    "    )\n",
    "    | RAG_PROMPT\n",
    "    | llm # Le LLM HuggingFacePipeline est utilisé ici\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- TESTS DE VALIDATION ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST DE LA CHAÎNE RAG DIRECTE SUR COLAB (Validation Projet 2)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test 1: Question Interne (DOIT utiliser le RAG pour répondre)\n",
    "print(\"\\n--- TEST 1: Question Interne ---\")\n",
    "question_interne = \"Combien de temps faut-il pour finaliser le traitement d'un remboursement ?\"\n",
    "print(f\"Question : {question_interne}\")\n",
    "response_interne = rag_chain.invoke({\"question\": question_interne})\n",
    "print(f\"\\nRÉPONSE RAG : {response_interne}\")\n",
    "\n",
    "# Test 2: Question Généraliste (DOIT répondre 'Information non disponible')\n",
    "print(\"\\n--- TEST 2: Question Générale ---\")\n",
    "question_generale = \"Quel est le plus grand désert du monde ?\"\n",
    "print(f\"Question : {question_generale}\")\n",
    "response_generale = rag_chain.invoke({\"question\": question_generale})\n",
    "print(f\"RÉPONSE RAG : {response_generale}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
