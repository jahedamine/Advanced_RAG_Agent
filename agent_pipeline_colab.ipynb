{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2645be98",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# üß† Advanced RAG Agent ‚Äî Documentation Interne\n",
    "Ce notebook construit un agent RAG capable de r√©pondre √† des questions sur une documentation interne, en utilisant FAISS, LangChain LCEL, et Mistral-7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b42d8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Installation des biblioth√®ques n√©cessaires:\n",
    "!pip install -q langchain transformers sentence-transformers faiss-cpu accelerate torch langchain_community langchain-huggingface langchain-core\n",
    "# pour utilis√© ChromaDB:\n",
    "# pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©l√©chargement des d√©pendances Python:\n",
    "import os\n",
    "import torch\n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"Installations et imports termin√©s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01820fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION DES COMPOSANTS RAG :\n",
    "\n",
    "FILE_PATH = \"documentation_interne.txt\"\n",
    "\n",
    "# Chargement et Division des donn√©es (Chunking):\n",
    "try:\n",
    "    with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        document_content = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERREUR: Le fichier {FILE_PATH} n'a pas √©t√© trouv√©. Veuillez le t√©l√©verser dans Colab.\")\n",
    "    exit()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "docs = text_splitter.create_documents([document_content])\n",
    "print(f\"Document divis√© en {len(docs)} segments (chunks).\")\n",
    "\n",
    "# EMBEDDING (Hugging Face Local et Gratuit):\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'} \n",
    ")\n",
    "\n",
    "# CR√âATION DU VECTOR STORE (M√©moire RAG):\n",
    "vector_store = FAISS.from_documents(docs, embedding_function)\n",
    "rag_retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"‚úÖ Base de donn√©es FAISS (RAG) cr√©√©e avec succ√®s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%writefile rag_chain.py\n",
    "# Charger Mistral 7B Instruct depuis Hugging Face:\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Utiliser le pipeline Transformers (GPU de Colab)\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        # 'device' est retir√© ici pour √©viter le conflit\n",
    "    },\n",
    "    device=0,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16} \n",
    ")\n",
    "print(f\"‚úÖ LLM Agent (Mistral-7B-Instruct) charg√© depuis Hugging Face sur Colab.\")\n",
    "\n",
    "\n",
    "# --- CONSTRUCTION DE LA CHA√éNE RAG DIRECTE (LCEL) ---\n",
    "\n",
    "RAG_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
    "Tu es un assistant IA professionnel. Utilise UNIQUEMENT le contexte fourni ci-dessous pour r√©pondre √† la question. \n",
    "Si la r√©ponse n'est pas dans le contexte, tu dois r√©pondre de mani√®re polie : 'Je suis d√©sol√©, cette information sp√©cifique n'est pas disponible dans ma documentation interne.'\n",
    "\n",
    "--- CONTEXTE ---\n",
    "{context}\n",
    "--- FIN CONTEXTE ---\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "# Assemblage de la Cha√Æne RAG LCEL\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context= (lambda x: x['question']) | rag_retriever \n",
    "    )\n",
    "    | RAG_PROMPT\n",
    "    | llm # Le LLM HuggingFacePipeline est utilis√© ici\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- TESTS DE VALIDATION ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST DE LA CHA√éNE RAG DIRECTE SUR COLAB (Validation effectu√©e)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test 1: Question Interne (utilisation du RAG pour r√©pondre)\n",
    "print(\"\\n--- TEST 1: Question Interne ---\")\n",
    "question_interne = \"Combien de temps faut-il pour finaliser le traitement d'un remboursement ?\"\n",
    "print(f\"Question : {question_interne}\")\n",
    "response_interne = rag_chain.invoke({\"question\": question_interne})\n",
    "print(f\"\\nR√âPONSE RAG : {response_interne}\")\n",
    "\n",
    "# Test 2: Question G√©n√©raliste (DOIT r√©pondre 'Information non disponible')\n",
    "print(\"\\n--- TEST 2: Question G√©n√©rale ---\")\n",
    "question_generale = \"Quel est le plus grand d√©sert du monde ?\"\n",
    "print(f\"Question : {question_generale}\")\n",
    "response_generale = rag_chain.invoke({\"question\": question_generale})\n",
    "print(f\"R√âPONSE RAG : {response_generale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from rag_chain import rag_chain  # Assure-toi que rag_chain est export√© dans rag_chain.py\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Query(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "async def ask_question(query: Query):\n",
    "    \"\"\"\n",
    "    Pose une question √† l'agent RAG.\n",
    "    - **question**: Texte de la question √† poser\n",
    "    - **return**: R√©ponse g√©n√©r√©e par le mod√®le, bas√©e sur la documentation interne\n",
    "    \"\"\"\n",
    "    response = rag_chain.invoke({\"question\": query.question})\n",
    "    return {\"answer\": response}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    return {\"status\": \"ok\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca9f93",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyngrok\n",
    "!nohup uvicorn app:app --host 0.0.0.0 --port 8000 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"üîó URL publique de ton API :\", public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de64d3fb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!ngrok config add-authtoken #YOUR_NGROK_AUTH_TOKEN"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
