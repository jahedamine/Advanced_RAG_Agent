# ğŸ¤– Advanced RAG Agent (Retrieval-Augmented Generation)

Ce dÃ©pÃ´t contient le code du **Projet** : Construction d'un Agent RAG (Retrieval-Augmented Generation) avancÃ© pour interroger une documentation interne Ã  l'aide d'un Large Language Model (LLM) en environnement local et/ou cloud.

Le projet a Ã©tÃ© validÃ© en utilisant l'architecture moderne **LCEL (LangChain Expression Language)** pour construire une chaÃ®ne RAG performante et prouver la capacitÃ© du systÃ¨me Ã  filtrer les connaissances gÃ©nÃ©rales.

---

## ğŸ§  Architecture du SystÃ¨me

Le pipeline RAG est structurÃ© autour de **trois composants principaux**, visant Ã  fournir des rÃ©ponses prÃ©cises et contextualisÃ©es :

- **RÃ©cupÃ©ration (Retrieval)**  
  Les donnÃ©es sont encodÃ©es Ã  l'aide des embeddings `all-MiniLM-L6-v2` (Hugging Face) et stockÃ©es dans une base de donnÃ©es vectorielle **FAISS** (utilisÃ©e dans Colab).

- **LLM (Large Language Model)**  
  Le modÃ¨le `Mistral-7B-Instruct` (via Hugging Face Pipeline sur GPU Colab) est utilisÃ© pour le raisonnement.

- **ChaÃ®ne LCEL**  
  Le **LangChain Expression Language** assemble le Retriever et le LLM pour forcer le modÃ¨le Ã  rÃ©pondre uniquement avec le contexte rÃ©cupÃ©rÃ©, validant ainsi la compÃ©tence RAG.

---

## âœ… Validation du Projet (Google Colab)

En raison des contraintes de mÃ©moire (RAM/VRAM insuffisante pour les gros modÃ¨les) sur l'environnement local, le projet a Ã©tÃ© validÃ© avec succÃ¨s sur **Google Colab (GPU T4)**.

Le notebook `agent_pipeline_colab.ipynb` prouve la bonne exÃ©cution du pipeline Ã  travers **deux tests critiques** :

- **Question Interne (SuccÃ¨s RAG)**  
  Le LLM rÃ©pond correctement aux questions basÃ©es sur le contenu de `documentation_interne.txt`.

- **Question GÃ©nÃ©rale (Ã‰chec contrÃ´lÃ©)**  
  Le LLM refuse de rÃ©pondre Ã  une question hors-sujet, prouvant l'efficacitÃ© du mÃ©canisme de filtrage du RAG.

---

## ğŸš€ Comment ExÃ©cuter le Projet

### ğŸ”‘ Fichiers ClÃ©s

- `agent_pipeline_colab.ipynb` : Notebook de validation fonctionnel (mÃ©thode recommandÃ©e)  
- `documentation_interne.txt` : Fichier source de la documentation  
- `requirements.txt` : Liste des dÃ©pendances Python

### ğŸ§ª Instructions Colab

1. Ouvrir le fichier `agent_pipeline_colab.ipynb` dans [Google Colab](https://colab.research.google.com).
2. Activer l'accÃ©lÃ©rateur matÃ©riel **T4 GPU**.
3. TÃ©lÃ©verser le fichier `documentation_interne.txt` dans la racine du notebook.
4. ExÃ©cuter toutes les cellules sÃ©quentiellement.

---

## ğŸ“‚ Auteur

Projet rÃ©alisÃ© par **Amine**, ingÃ©nieur GenAI spÃ©cialisÃ© en alignement, agentique et dÃ©ploiement local/cloud.  
Ce projet fait partie dâ€™un programme de consolidation en 5 modules GenAI Engineering.

---

Tu veux que je tâ€™aide Ã  rÃ©diger la section "License" ou "Contributions" pour complÃ©ter ton dÃ©pÃ´t ?  
**Ce README est plus quâ€™un fichier â€” câ€™est la vitrine de ton agent cognitif.** ğŸ§ ğŸ“‚ğŸš€
