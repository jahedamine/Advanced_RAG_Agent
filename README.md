# Advanced RAG Agent (Retrieval-Augmented Generation)

Ce d√©p√¥t contient le code du **Projet** ADVANCED_RAG_AGENT : Agent RAG avanc√© capable de r√©pondre √† des questions complexes √† partir de documents internes (PDF, DOCX, TXT‚Ä¶). Contrairement aux chatbots classiques, il utilise une architecture Retrieval-Augmented Generation (RAG) avec LangChain, recherche vectorielle et LLM pour g√©n√©rer des r√©ponses pr√©cises, contextuelles et modulaires. Expos√© en API et Dockeris√©, il incarne une intelligence documentaire pr√™te √† l‚Äôemploi.
Le projet a √©t√© valid√© en utilisant l'architecture moderne LCEL (LangChain Expression Language) pour construire une cha√Æne RAG performante et prouver la capacit√© du syst√®me √† filtrer les connaissances g√©n√©rales.

---

## Architecture du Syst√®me

Le pipeline RAG est structur√© autour de **trois composants principaux**, visant √† fournir des r√©ponses pr√©cises et contextualis√©es :

- **R√©cup√©ration (Retrieval)**  
  Les donn√©es sont encod√©es √† l'aide des embeddings `all-MiniLM-L6-v2` (Hugging Face) et stock√©es dans une base de donn√©es vectorielle **FAISS** (utilis√©e dans Colab).

- **LLM (Large Language Model)**  
  Le mod√®le `Mistral-7B-Instruct` (via Hugging Face Pipeline sur GPU Colab) est utilis√© pour le raisonnement.

- **Cha√Æne LCEL**  
  Le **LangChain Expression Language** assemble le Retriever et le LLM pour forcer le mod√®le √† r√©pondre uniquement avec le contexte r√©cup√©r√©, validant ainsi la comp√©tence RAG.

---

## Validation du Projet (Google Colab)

En raison des contraintes de m√©moire (RAM/VRAM insuffisante pour les gros mod√®les) sur l'environnement local, le projet a √©t√© valid√© avec succ√®s sur **Google Colab (GPU T4)**.

Le notebook `agent_pipeline_colab.ipynb` prouve la bonne ex√©cution du pipeline √† travers **deux tests critiques** :

- **Question Interne (Succ√®s RAG)**  
  Le LLM r√©pond correctement aux questions bas√©es sur le contenu de `documentation_interne.txt`.

- **Question G√©n√©rale (√âchec contr√¥l√©)**  
  Le LLM refuse de r√©pondre √† une question hors-sujet, prouvant l'efficacit√© du m√©canisme de filtrage du RAG.

---

## Fonctionnalit√©s
- Chunking + vectorisation locale via FAISS
- R√©cup√©ration contextuelle avec LangChain
- G√©n√©ration de r√©ponse via Mistral-7B-Instruct
- Exposition API via FastAPI
- Dockerisation pour d√©ploiement

## Comment Ex√©cuter le Projet

### Fichiers Cl√©s

- `agent_pipeline_colab.ipynb` : Notebook de validation fonctionnel (m√©thode recommand√©e)  
- `documentation_interne.txt` : Fichier source de la documentation  
- `requirements.txt` : Liste des d√©pendances Python

### Instructions Colab

1. Ouvrir le fichier `agent_pipeline_colab.ipynb` dans [Google Colab](https://colab.research.google.com).
2. Activer l'acc√©l√©rateur mat√©riel **T4 GPU**.
3. T√©l√©verser le fichier `documentation_interne.txt` dans la racine du notebook.
4. Ex√©cuter toutes les cellules s√©quentiellement.

---

## üêç D√©ploiement local
```bash
pip install -r requirements.txt
uvicorn app:app --reload